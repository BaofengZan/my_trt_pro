# 实现Tenosor的管理

`git checkout v2.1`

我们对tensor的需求主要有：

1. tensor管理
   
   1. tensor的维度管理
      
      * 因为还涉及到计算byte数，所以在构造时，需要将当前tensor的类型也传进来。
   
   2. tensor的head管理：当前数据是处于gpu上还是cpu上。
   
   3. tensor的resize，~~重分配管理~~
      
      * ~~只有当现有的size 大于之前的size时，才会分配~~
      
      * 但是当重resize的大小大于现有的时，要将head 置为初始状态
   
   4. tensor根据偏移量取值 ： 多维tensor
      
      - 提供索引，计算在一维数组中的偏移量
   
   5. tensor的数据管理
      
      - 显存分配，释放
        
        - 显示调用 cpu gpu函数时 ，自动调用
          
          - 判断当前
      
      - gpu和cpu的数据拷贝（自动）
        
         a. 根据当前head是在gpu上还是cpu上，来判断是否需要拷贝
      
      - 根据偏移拿值
        
        - ```
          float* cpu_out = (float*)output_array_device.cpu();
          cpu_out + 1 + i * NUM_BOX_ELEMENT
          ```
        
        - 比如这两句，可以直接在tensor类中合并成一句  `output_array_device.cpu(1 + i * NUM_BOX_ELEMENT)` 拿到的就是当前偏移的起始位置
          
          - 此时还只能兼容1个batch的情况
          
          - 还涉及到不同的type，所以使用template
      
      - tensor的赋值
        
        - 手动给tensor赋值
          
          - data_cpu2gpu  data_gpu2gpu
      * 我们要禁止tensor的拷贝
        
        * 我们不希望：对tensor的所有操作都是针对同一个对象的，所以禁止拷贝。只能使用引用操作。（对象语义）
   
   6* workspace的概念
   
   * 这个空间，是一个随时可以被使用的gpu空间，只有在需要的时候，才会分配。
   
   * 这个空间是Tenor的一部分。
   
   * ```
         uint8_t* img_device = nullptr;
         int  size_image = input.cols * input.rows * 3;  // uint8 sizeof=1
         checkCudaRuntime(cudaMalloc((void**)&img_device, size_image));
         cudaMemcpy(img_device, input.data, size_image, cudaMemcpyHostToDevice);
     ```
   
   * 上面几句是每次预处理时都会使用的一块显存，可以使用workspace的显存

现在v1版本中tensor都是手动分配 释放以及copy的，我们将这些函数封装到tensor类中。

# 代码复用

我们在tensor中，对于gpu/cpu/workspace的size和维度以及data的管理实际上都是重复的，没有用到“代码复用”。 因此可以将对内（显）存的管理再次封装一下。

这里没有代码实现，但是思路很清晰。